\documentclass[10pt, conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{bm}
\usepackage{subcaption}
\usepackage{float}
\usepackage[acronym]{glossaries}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}


\makeglossaries

\newacronym{ann}{ANN}{all k-nearest neighbour}
\newacronym{cews}{CEWS}{cut edges weight statistic}
\newacronym{cart}{CART}{classification and regression trees}
\newacronym{eda}{EDA}{exploratory data analysis}
\newacronym{enn}{ENN}{edited nearest neighbour}
\newacronym{id3}{ID3}{iterative dichotomiser 3}
\newacronym{iqr}{IQR}{Interquartile Range}
\newacronym{knn}{KNN}{k-nearest neighbours}
\newacronym{mar}{MAR}{missing at random}
\newacronym{mcar}{MCAR}{missing completely at random}
\newacronym{nan}{NaN}{not a number}
\newacronym{ni}{NI}{nonignorable}
\newacronym{renn}{RENN}{repeated edited nearest neighbour}
\newacronym{smote}{SMOTE}{synthetic minority oversampling technique}


\begin{document}

\title{Assignment 1\\
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
% should not be used}
% \thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Andre van der Merwe}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
}

\maketitle

\begin{abstract}
    
\end{abstract}

\section{Introduction}

\begin{itemize}
    \item Context: Begin by explaining the importance of data quality in machine learning and the relevance of classification algorithms like KNN and classification trees in the context of the DryBeanDataSet.
    \item Goals: State that the main goal is to identify and address data quality issues and develop two predictive models to classify dry beans.
    \item Approach: Briefly outline the steps taken: characterizing the dataset, identifying and addressing data quality issues, applying the KNN and classification tree algorithms, and evaluating their performance.
    \item Motivation: Explain why this study is important, such as improving the accuracy of classification models in agricultural datasets.
    \item Main Observations: Summarize the key findings, such as which model performed better and how data quality issues impacted the results.
    \item Outline: Provide a brief overview of the structure of the report, mentioning the sections that follow.
\end{itemize}

\section{Background}

This section presents background information on \\ \acrlong{eda} and techniques used to clean and transform data
for analysis or to construct models. Additionally, information on the C4.5 decision tree and k-nearest neighbours
classification models.

\subsection{Exploratory Data Analysis}\label{EDA_background}

Exploratory data analysis \acrshort{eda} first introduced by John Tukey in 1977 \cite{EDA_ref}. Tukey
emphasised that there are three key strategies in data analysis, namely:
\begin{itemize}
    \item Grapical presentation: the use of visual tools, such as plots, to explore, analyse and understand data.
    \item Flexibility in viewpoint and facilities: encourages creative and diverse approaches to analyse data
          that allows for dynamic perspective and adaptive tools.
    \item Search for parsemony and simplicity: To find simple and clear explanations in the data while
          unnecessary complexity is avoided.
\end{itemize}

\subsubsection{Data Quality Issues}

\paragraph{Missing values}

The absence of data in a dataset where a value should be present is known as a missing value. There are
three types of missing values, namely \acrfull{mar}, \acrfull{mcar} and \acrfull{ni} missing values \cite{Missing_ref}.

\acrshort{mcar} refers to a situation where missing values are distributed randomly throughout the dataset, independent
of both observed and unobserved data. Under \acrshort{mcar}, modern methods such as missing value imputation can be
used and still produce unbiased estimates, though some uncertainty is introduced. When modern methods are used
to handel \acrshort{mcar} missing values, the statistical power reduces compared to complete data.

A missing value is \acrshort{mar} if the likelihood of the missing data on the variable is unrelated to the value of
that variable itself, once other variables are taken into account. These other features help explain why data may
be missing and is known as mechanisms. Some common mechanisms include factors like education, income or age.
The \acrshort{mar} assumption holds if the pattern of missingness is random when explained by the mechanisms.

If data is missing in a systematic way and can neither be classified as \acrshort{mar} or \acrshort{mcar}, then
is the missing value classified as \acrshort{ni}. To model this type of data is complex.

There are three ways to deal with these missing values. The first option explored is to use machine learning
algorithms that is robust to missing values. The second option is to remove the instances that contains the
missing values. This option does run the risk of valuable information that may be lossed. The third option
is to impute the missing values.

When a missing value is imputed from a categorical feature, the most frequently occuring value, known as the mode,
is commonly used. When a numerical feature is imputed, the median is typically used when the data contains outliers
and the mean is used when the data contains no outliers. 

\paragraph{Outliers}

When one or more variables, of an instance in the dataset, have values that significantly differ from the overall distribution
of the data, it is known as an outlier. There are two types of outliers, namely invalid outliers that is data points included
in the dataset due to errors, and valid outliers that is correct values that are legitimately different from the rest
of the data.

There are three techniques to cope with outliers. The first technique is to remove the outliers using statistical
techniques. The second technique is to keep the outliers and apply robust estimators that are less sensitive to the
influence of the outliers. Alternatively, clipping can be performed to the outliers to limit the range of the outliers
or to impute a value to replace the outlier with. The third technique is to remove the outliers directly within the
machine learning process by the use of algorithms designed to be robust to outliers.

One approach to perform outlier detection is by the use of a statistical method known as the \acrfull{iqr} method.
The equation to calculated the \acrshort{iqr} of an input feature $\boldsymbol{\textbf{p}}$, is as follows:
\begin{equation}
    IQR^{\boldsymbol{\textbf{p}}} = Q_3^{\boldsymbol{\textbf{p}}} - Q_1^{\boldsymbol{\textbf{p}}} \label{IQR}
\end{equation}
where $Q_1^{\boldsymbol{\textbf{p}}}$ and $Q_3^{\boldsymbol{\textbf{p}}}$ is the first and third quartile of the input
feature ${\boldsymbol{\textbf{p}}}$ respectively. The equations to determine if an instance of ${\boldsymbol{\textbf{p}}}$
is an outlier, is as follows:
\begin{equation}
    \boldsymbol{\textbf{p}}_i < Q_1^{\boldsymbol{\textbf{p}}} - 1.5 \times IQR^{\boldsymbol{\textbf{p}}} \label{outlier_smaller}
\end{equation}
or if:
\begin{equation}
    \boldsymbol{\textbf{p}}_i > Q_3^{\boldsymbol{\textbf{p}}} + 1.5 \times IQR^{\boldsymbol{\textbf{p}}} \label{outlier_larger}
\end{equation}
where $\boldsymbol{\textbf{p}}_i$ is the $i$-th instance of the input feature ${\boldsymbol{\textbf{p}}}$.

Another technique to apply outlier detection is by using visual methods such as a boxplot, that displays the
distribution of data based on quartiles and highlights potential outliers, or by using a scatter plot to visualise
the the relationship between two features and can reveal outliers as points that deviate significantly from the
overall pattern.

\paragraph{Noise}

Noise refers to random or irrelevant variations in the data that can obscure patterns. There is two types of noise.
The first typeof noise is stochastic noise, with random variation in data values, a zero mean in the variation and
small deviations. If the deviation of stochastic noise is large, the feature becomes irrelevant. The second type
of noise is systematic noise, where all values shifted in a particular direction, such as due to poorly calibrated
instruments.

When handeling noise, systematic noise can be corrected by use of a few different techniques. One technique is to
make use of standardisation. Stochastic noise is handled by the machine learning process, but caution should be
used as if noise is present and the model is too complex, the model will overfit.

There are a few techniques used to remove noise in the dataset, namely \acrfull{enn}, \acrfull{renn}, \acrfull{ann},
\acrfull{cews} and Tomek links. A Tomek link is a pair of data instances $\boldsymbol{\textbf{a}}$ and
$\boldsymbol{\textbf{b}}$ where there exists a third instance $\boldsymbol{\textbf{c}}$ such that:
\begin{equation}
    d(\boldsymbol{\textbf{a}}, \boldsymbol{\textbf{c}}) \leq d(\boldsymbol{\textbf{a}}, \boldsymbol{\textbf{b}}) \label{tomek_1}
\end{equation}
or
\begin{equation}
    d(\boldsymbol{\textbf{b}}, \boldsymbol{\textbf{c}}) \leq d(\boldsymbol{\textbf{a}}, \boldsymbol{\textbf{b}}) \label{tomek_2}
\end{equation}
where $d$ is the distance between the two instances.

If $\boldsymbol{\textbf{a}}$ and $\boldsymbol{\textbf{b}}$ forma Tomek link, then either one of them is noise
or both are borderline cases. In such cases both $\boldsymbol{\textbf{a}}$ and $\boldsymbol{\textbf{b}}$
is removed to improve the quality of the dataset.

\paragraph{Imbalanced class data}

A dataset is refered to as imbalanced, if the target feature has significantly unequal
representation among its different categories. A few approaches to handel imbalanced classes are, do to
nothing, but this is not a great solution, as the train and test accuracy of the majority class may be
very good and the train and test accuracy of the minority class will be bad. Another approach to
handel imbalanced classes is to balance the training set by either the use of \acrfull{smote}
to oversample the minority class or the use of Tomek links to undersampl the majority class.

By using Tomek links as described in Equation \eqref{tomek_1} and Equation \eqref{tomek_2}, it is possible
to obtain Tomek links between classes and to then remove the majority class from each Tomek link.

\acrshort{smote} is used to oversample the minority class. The process of \acrshort{smote} is to
firstly ignore the majority class. The next step is to choose the \acrshort{knn} for each minority
instance. \acrshort{smote} then produces new instances halfway between the the instance and the
neighbours.

\subsubsection{Data Preparation}

\paragraph{Data type transformations}

Different machine learning algorithms have varying requirements for the types of input data. This necessitates
the conversion of the data from one type to another.

Discretisation is the process of converting continuous features to discrete features by the creation of bins
that group the continuous values into distinct categories. Each bin represents a specific range derived from the
the original continuous features. There is a general trade-off when a decision has to be made about the number of
bins to use. This trade off is that valuable information may be lossed, if the number of bins is very low and
if the number of bins is high, there might be very few instances in each bin or a bin could end up being empty.

There are typically two methods to encode discrete feature as a numerical Feature. The first method is known as
ordinal encoding. Ordinal encoding transforms discrete features that typically has a natural order into continuous
features. An example of this is that if the unique values of a discrete feature is cold, mild and high will be
encoded as 0, 1 and 2 respectively.

One hot encoding is typically used when there is no natural order of the unique values of the discrete feature.
If the unique values are cloudy, sunny and rainy, the values will be encoded as 1 0 0 for cloudy, 0 1 0 for
sunny and 0 0 1 for rainy.


\paragraph{Feature selection}

The goal of any feature selection method is to find the smallest subset of informative features that
preserves the overall performance of the model. A smaller subset of features is beneficial as the
models fitted to the data will be simpler. The smaller set of features will also produce shorter
train times and produce less overfitted models, therefore improving the generalisation of a model.

If the subset of features contains too few features, the model may underfit due to the lack of
discriminant power to differentiate between classes. An underfitted model leads to more false
positives and false negatives. Too many features in the new subset of features can introduce
noise in the training data, causing the model to overfit and increase computational complexity.

There are four types of different descriptive features to distinguish from, namely:
\begin{itemize}
    \item \textbf{Predictive} descriptive feature that provides useful information.
    \item \textbf{Interacting} descriptive feature is not informative about the target feature on its own,
          but can provide valuable information when combined with one or more other features.
    \item \textbf{Redundant} descriptive feature that has a strong correlation with another feature.
    \item \textbf{Irrelevant} descriptive feature that does not provide any useful information when a value
           is estimated for the target feature.
\end{itemize}
Ideally, after a feature selection approach is applied, will the subset of features include the predictive and
interacting features while the irrelevant and redundant features are excluded.

There are three methods to perform subset selection on a dataset, namely filter methods, wrapper methods and
embedded methods. Filter methods performs statistical tests between input feature and target feature to
identify a subset of features that are more relevant than others. The statistical test to apply types
of input features to a specific type of target feature is represented by Table \ref{tab:feature_selection_table}.
\begin{table}[h!]
    \caption{Statistical Tests for Evaluating Different Types of Input Features Against a Specific Type of Target Feature}
    \begin{center}
        \begin{tabular}{|c||c|c|}
            \hline
            \textbf{Feature}&\multicolumn{2}{|c|}{\textbf{Target Feature Type}} \\
            \cline{2-3} 
            \textbf{Type} & \textbf{\textit{Continuous}}& \textbf{\textit{Categorical}} \\
            \hline
            \textbf{\textit{Continuous}}& Pearson Correlation& LDA\\
            \hline
            \textbf{\textit{Categorical}}& Anova & Chi-Square\\
            \hline
            % \multicolumn{3}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
        \end{tabular}
        \label{tab:feature_selection_table}
    \end{center}
\end{table}

Two common approaches to apply the wrapper feature selection technique is by the use of forward and backward
feature selection that produces a best subset of features to train the model on. In forward and backward feature
selection a feature is added or removed to the best subset of features respectively and then the performance
of the model is compared when using the best subset of features and the best subset of features that has been
modified.

Embedded subset selection techniques is when an algorithm has a build in feature to perform subset selection.
Models like decision trees prune away splits on features that has little to no predictive power and techniques
like regularisation is applied to the objective function of a model to reduce the predictive
power of features that has little to no influence when predicting a target feature.

\paragraph{Normalisation}

Normalisation or scaling of input features is required for some machine learning algorithms. Normalisation
is typically required if the ranges of values for different input features differ in order of magnitudes 
and where the algorithm makes use of a distance based metric to generate classification
models. Normalisation is also used when the minimum and maximum values are not known, to
reduce the effect of outliers or to transform a range of values to a different range of values. One method
of normalisation typically used is the Z-score normalisation also known as standardisation. The equation to
perform standardisation is as follows:
\begin{equation}
    Z_{i,j} = \frac{x_{i,j} - \mu_j}{\sigma_j} \label{standardisation}
\end{equation}
where $x_{i,j}$ is the data value of the $j$-th feature for the $i$-th observation, $\mu_j$ and $\sigma_j$
is the mean and standard deviation of the $j$-th feature across all observations respectively and $Z_{i,j}$
is the Z-score of the data value at the $i$-th observation and the $j$-th feature.

Standardisation transform all of the values of each feature to have a zero-mean and a unit variance.

\subsection{K-Nearest Neighbours}\label{KNN_background}

The \acrfull{knn} algorithm was first propsed by Evelyn Fix and Joseph Hodges in 1951 \cite{KNN_ref}
in a technical report that was never published. The report contained pioneering work on nonparametric
discriminant analysis and probability density estimation and laid the foundational principles for
the \acrshort{knn} algorithm.

\acrshort{knn} is a type of instance-based learning that does not build a model based on the training
dataset. Instead, \acrshort{knn} computes the distance between the instances in the test set and all
of the instances in the training set to identify the nearest neighbours for each entry in the test set.
\acrshort{knn} relies heavily on the distance metric in order to identify the instances in the training
set closest to each of the test instances. The distance metric plays a crucial role in order to determine
the similarity between data points, that directly influences the performance and accuracy of the algorithm.
The most common distance metrics used in the \acrshort{knn} algorithm is the:
\begin{itemize}
    \item Euclidean distance metric
    \item Manhattan distance metric
    \item Minkowski distance metric
\end{itemize}

The Euclidian distance metric equation is defined as follows:
\begin{equation}
    d(\boldsymbol{\textbf{x}}_1, \boldsymbol{\textbf{x}}_2) = \sqrt{\sum_{n=1}^{N}(x_{1n} - x_{2n})^2} \label{euclidian}
\end{equation}
The Manhattan distance metric equation is defined as follows:
\begin{equation}
    d(\boldsymbol{\textbf{x}}_1, \boldsymbol{\textbf{x}}_2) = \sum_{n=1}^{N} \left| x_{1n} - x_{2n} \right| \label{manhattan}
\end{equation}
The Minkowski distance metric equation is defined as follows:
\begin{equation}
    d(\boldsymbol{\textbf{x}}_1, \boldsymbol{\textbf{x}}_2) = \left(\sum_{n=1}^{N} \left| x_{1n} - x_{2n} \right|^p\right)^\frac{1}{p} \label{minkowski}
\end{equation}
where $\boldsymbol{\textbf{x}}_1$ and $\boldsymbol{\textbf{x}}_2$ is a vector containing all the of the features of two
distinct instances in a dataset, $d(\boldsymbol{\textbf{x}}_1, \boldsymbol{\textbf{x}}_2)$ is the distance between the two
vectors $\boldsymbol{\textbf{x}}_1$ and $\boldsymbol{\textbf{x}}_2$, $N$ denotes the dimension of the vectors 
$\boldsymbol{\textbf{x}}_1$ and $\boldsymbol{\textbf{x}}_2$ or equivalently the total number of features and $x_{1n}$ and
$x_{2n}$ is the value of the $n$-th feature of the two distinct feature vectors $\boldsymbol{\textbf{x}}_1$ and
$\boldsymbol{\textbf{x}}_2$ respectively.

After the distances between the instance from the test set is calculated against all of the instances in the training set,
a distance vector $\boldsymbol{\textbf{d}}$ is obtained. The distance vector is then sorted from smallest to largest distances
and the first $k$ distances is chosen from $\boldsymbol{\textbf{d}}$. The process to classify an unseen instance,
$\boldsymbol{\textbf{x}}$, on $k$ of the instances of the training set $D$ is described by Algorithm \ref{alg:KNN_algorithm}.
\begin{algorithm}
\caption{k-Nearest Neighbours (kNN)}
\label{alg:KNN_algorithm}
\begin{algorithmic}[1]
    \Function{kNN}{$D$, $\boldsymbol{\textbf{x}}$, $k$}
        \ForAll{$\boldsymbol{\textbf{x}}_i \in D$}
            \State $\boldsymbol{\textbf{d}}$ = \Call{distance}{$\boldsymbol{\textbf{x}}_i, \boldsymbol{\textbf{x}}$}
        \EndFor
        \State \Call{sort}{$\boldsymbol{\textbf{d}}$}
        \State $S$ = set of $k$ patterns in $D$ closest to $\boldsymbol{\textbf{x}}$
        \State Return class as majority class in $S$
    \EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{C4.5 Decision Trees}\label{CT_background}

The first implementation of a decision tree algorithm was introduced by Leo Brieman \textit{et al} in 1984 and
is know as \acrfull{cart} \cite{CART_ref}. In 1986 Ross Quinlan implemented the \acrfull{id3} decision tree
algorithm, which is able to cope with noise and missing values in the data \cite{ID3_ref}. In 1993 Ross Quinlan
introduced the C4.5 decision tree algorithm, which is able to handle continuous features, imbalanced classes and
introduced post-pruning \cite{C4.5_ref}.

C4.5 generates a classification tree model that consists of:
\begin{itemize}
    \item Leaf nodes that indicates different discrete classes
    \item Decision nodes that represent tests on a feature and leads to further branches in the tree.
\end{itemize}
C4.5 is induced to overfit the training data, thus the tree classifier completely separates the classes of the
training data and leads to poor generalisation on usneen data as well as outliers and noise will be in the leaves of
the tree.

C4.5 uses entropy as a measure of impurity in a dataset with respect to the target feature. The entropy of a
dataset will be 0, if all of the classes in the dataset has the same labels. If the frequency of the
classes are all equal, then will the entropy of the dataset be equal to 1. The test on an input feature
that minimises the impurity of a dataset, or equivalently maximises the information gain, is selected as the
best possible feature to split on. The equation to calculate the entropy of a dataset is as follows:
\begin{equation}
    H(D)=-\sum_{i=1}^{M} p(y_m)log_M (p(y_m))\label{entropy}
\end{equation}
and:
\begin{equation}
    p(y_m) = \frac{freq(y_m,D)}{\left\lvert D \right\rvert }\label{class_prob} 
\end{equation}
where $D$ represents the dataset, $y_m$ is the $m$-th class, $M$ is the total number of distinct classes in the
dataset, $p(y_m)$ is the probability of a class $y_m$ occuring in $D$ and where $freq(Y_m,D)$ is the number of
times class $y_m$ occurs in D.

The equation used to calculate the entropy due to the split, follows as:
\begin{equation}
    H_x(D) = \sum_{o=1}^{O} p_o H(D_o)\label{entropy_x}
\end{equation}
and:
\begin{equation}
    p_o = \frac{\left\lvert D_o \right\rvert}{\left\lvert D \right\rvert}\label{o_prob}
\end{equation}
where $x$ is the input variable used to split $D$, $O$ is the number of outcomes for $x$, $H(D_o)$ is the copnditional entropy of dataset
$D_o$ with reference to the class distribution and where $p_o$ is the probability of outcome $o$.

The equation used to calculate the information gain if $D$ is partitioned on $x$, follows as:
\begin{equation}
    gain(x) = H(D) - H_x(D)\label{info_gain}
\end{equation}
The information gain calculation favours input features with many outcomes and introduces bias as the tree
creates multiple branches, one for each unique value. The solution to the bias is to normalise the information gain with entropy
with respect to test outcomes. The equation for the gain ratio is as follows:
\begin{equation}
    gainRatio(x) = (1-F)\frac{gain(x)}{splitInfo(x)}\label{gainRatio}
\end{equation}
and:
\begin{equation}
    splitInfo(x) = -\sum_{o=1}^{O}p_o log_O(p_o)\label{splitInfo}
\end{equation}
where $F$ is the fraction of patterns in $D$ for which the value of $x$ is missing.
The objective of the tree is to select $x$ that maximise the gain ratio.

In order to handel numerical continuous features, C4.5 first sorts the number of unique values, denoted by $I$,
found in the numeric input feature $x$ to create the sequence, $(v_1,v_2,...,v_I)$ ordered from smallest to largest.
C4.5 performs $I$-1 splits between $v_i$ and $v_{i+1}$ and for each of these splits the information gain is calculated by use of
Equation \eqref{gainRatio}. The split with the highest calculated information gain will be chosen and the threshold to
split the tree on is calculated as follows:
\begin{equation}
    \epsilon = \frac{v_i + v_{i+1}}{2}\label{numeric}
\end{equation}
where $\epsilon$ is the threshold to split the tree on. C4.5 then generates two decision nodes with test $x < \epsilon$ and $x >= \epsilon$.

C4.5 partitions the dataset $D$ in the absence of missing values as follows: 
\begin{equation}
    w_j(D_o) = 1, w_j(D_q)=0, \forall q \neq o \label{no_missing}
\end{equation}
where each pattern $j$ in $D$ belongs to subset $D_o$ with weight $w_i(D_o)$, provided that pattern $j$ no missing value for the input
feature $x$. If pattern $j$ has a missing value for feature $x$, it is assigned to all outcomes with a weight given by:
\begin{equation}
    w_j(D_o) = w_j(D)p_o \label{with_missing}
\end{equation}
where the probability of the outcome is now calculated as follows:
\begin{equation}
    p_o = \frac{\Sigma_{x_j \epsilon D \text{ with outcome } _o} w_j(D)}{\Sigma_{x_j \epsilon D \text{ with any known outcome}} w_j(D)}
\end{equation}

To determine the optimal input feature $x$ to split the data on, the process follows these steps:
\begin{itemize}
    \item Calculate the entropy of the current dataset, $H(D)$ by use of Equation \eqref{entropy}
    \item Compute the conditional entropy for each input feature $x$, $H_x(D)$ by use of Equation \eqref{entropy_x}
    \item Calculate the information gain for each feature if $D$ would be split on $x$ by use of Equation \eqref{info_gain}
    \item Using the calculated information gain, calculate the gain ratio by use of Equation \eqref{gainRatio}
    \item Select the feature with the highest gain ratio and split $D$ into $O$ subsets
    \item Repeat the process on each subset until the tree has been induced to completely separate the classes
\end{itemize}

Algorithm \ref{alg:CTI_algorithm} describes the process to fully induce the decision tree. 
\begin{algorithm}
    \caption{Classification Tree Induction}
    \label{alg:CTI_algorithm}
    \begin{algorithmic}[1]
        \Function{InduceTree}{$D$}
            \If{$\left\lvert D \right\rvert$ = 0}
                \State Return leaf with default class
            \EndIf
            
            \If{$\left\lvert D \right\rvert$ $>$ 0 and $\forall \boldsymbol{\textbf{x}} \in D$ the class is the same, i.e. $y_m$}
                \State Return leaf with class label $y_m$, containing $D$
            \EndIf{}

            \State Select a test based on a single input variable
            \State Split $D$ into $D_1$,$D_2$,...,$D_o$, where $O$ is the number of outcomes
            \For{$o$ = 1 to $O$}
                \State \Call{InduceTree}{$D_o$}
            \EndFor
        \EndFunction
    \end{algorithmic}
\end{algorithm}

In order to overcome the issues of the fully induced tree that overfits the training data, as well as outliers and
noise in the leaves of the tree, C4.5 introduced post-pruning. C4.5 replaces the tests in the deepest level
of the tree with leaf nodes, where the prediction of the leaf node is the class that occurs the most in the new subset.
A validation set is used to test the generalisation performance of unseen data and if the generalisation performance
increases, the tree prunes away the decision node and replaces it with the leaf node. If the generalisation performance
does not improve, the decision node is then re-inserted into the tree. This process is repeated until no more decision
nodes are replaced. The tree is now robust to outliers and noise.

\section{Methodology}

This section provides the approach taken to explore and analyse the dry beans dataset to identify and address data quality
issues. Additionally, it details the methodology used to classify dry bean types through the use of a \acrshort{knn}
and classification tree model.

\subsection{Data Overview} \label{Data_Overview}

The structure and characteristics of the Dry Beans dataset are thoroughly investigated. The details on the number of instances
and types of features are investigated to provide a foundational understanding for the subsequent data analysis.

The Dry Bean dataset contains 13611 instances and 22 features. Among these features, 19 are numerical features and 3 are
categorical features, with the `Class' feature being the target feature. A closer examination reveals that
missing values in the dataset are represented by `?'. Missing values identified by `?' is very problematic as it changes
the continuous features containing missing values into discrete features. The dataset is modified so that each instance
of a missing value is changed from `?' to a value of \acrfull{nan}. The next step is to identify some important
characteristics of each feature and investigate these characteristics. The important characteristics of each
continuous feature are summarised in Table \ref{tab:continuous features}.

Several points of interest emerge after a closer examination of Table \ref{tab:continuous features}. The first
observation made is that the feature `Sort Order' feature has a cardinality of 13611, a minimum of approximately 0,
a first quartile value of approximately 0.25, a mean of approximately 0.5, a third quartile of approximately 0.75 and
a maximum value of approximately 1. These statistics all indicate that `Sort Order' is a unique identifier or index.
As a result, `Sort Order' may be considered as an irrelevant feature for predictive modeling purpose.

The second observation made is that feature `ConvexArea' contains a negative number as the minimum value. This is
impossible as area can not be negative. The boxplot of feature `ConvexArea' is given by Figure
\ref{ConvexArea_boxplot_negative} to further investigate the properties of the feature `ConvexArea'.
\begin{figure}[h!]
    \centerline{\includegraphics[scale=0.35]{../Plots/Negative ConvexArea boxplot.png}}
    \caption{Boxplot of feature ConvexArea vs feature Class}
    \label{ConvexArea_boxplot_negative}
\end{figure}

Figure \ref{ConvexArea_boxplot_negative} clearly shows that the negative value is an obscure outlier, thus can this
instance of the dataset be classified as an invalid outlier and should either be imputed or dropped from the dataset.

The third observation made is that both the minimum and the maximum values of feature `EquivDiameter' deviates
largely from the rest of the statistics. This large deviation may indicate that these instances of the dataset
are also invalid outliers. Figure \ref{EquivDiameter boxplot before} is the boxplot of feature `EquivDiameter'
that helps to investigate the obscure values of the instances in the dataset.
\begin{figure}[H]
    \centerline{\includegraphics[scale=0.35]{../Plots/EquivDiameter outlier boxplot.png}}
    \caption{Boxplot of feature EquivDiameter vs feature Class}
    \label{EquivDiameter boxplot before}
\end{figure}

From Figure \ref{EquivDiameter boxplot before} it is quite clear that the maximum value of feature `EquivDiameter'
is an invalid outlier. This instance is dropped from the dataset and the boxplot is generated again, as shown
in Figure \ref{EquivDiameter boxplot before 2}, to investigate the minimum value of the feature `EquivDiameter'.
\begin{figure}[h!]
    \centerline{\includegraphics[scale=0.35]{../Plots/EquivDiameter outlier boxplot 2.png}}
    \caption{Boxplot of feature EquivDiameter vs feature Class after the maximum value was dropped}
    \label{EquivDiameter boxplot before 2}
\end{figure}

As seen in Figure \ref{EquivDiameter boxplot before 2} there is another instance in the dataset that deviates
significantly from the rest of the observations and can also be classified as an invalid outlier. This outlier
is also removed from the dataset and the boxplot is again generated, as shown in Figure
\ref{EquivDiameter boxplot before 3}, to investigate the minimum value of the feature `EquivDiameter'.
\begin{figure}[H]
    \centerline{\includegraphics[scale=0.35]{../Plots/EquivDiameter outlier boxplot 3.png}}
    \caption{Boxplot of feature EquivDiameter vs feature Class after the invalid outliers were dropped}
    \label{EquivDiameter boxplot before 3}
\end{figure}

From Figure \ref{EquivDiameter boxplot before 3} it is clear that the minimum value is also an invalid outlier
that should either be imputed or removed from the dataset.

The important characteristics of each discrete feature in the Dry Bean dataset is summarised in Table
\ref{tab:categorical features}.

The most important observation made from Table \ref{tab:categorical features} is that the `Class' feature
is imbalanced, as the first and second modes together form 45\% of the data. The five other classes
makes up the remaining 55\%, and thus the `Class' feature is imbalanced.

\subsection{K-Nearest Neighbours}

\paragraph{Data preparation}

Data preparation is a very crucial step that must be completed before the \acrshort{knn} algorithm can be applied to the Dry
Bean dataset. If a \acrshort{knn} model is implemented as described in Section \ref{KNN_background}, then
will the \acrshort{knn} model be robust to missing values, noise and outliers in the dataset. To prepare
the Dry Bean dataset for a \acrshort{knn} model, the missing values have to be replaced with \acrshort{nan}
and the insatnces that contains missing values in the `Class' feature is dropped.

\acrshort{knn} is an instance-based learning algorithm that calculates the distance between feature
instances to classify new data points based on the majority class of their nearest neighbours. Therefore, all features need
to be numeric for the distance calculations to be valid. The `Colour' feature is a categorical feature with no
natural ordering and will therefore be one hot encoded.

This implementation of \acrshort{knn} is sensitive to imbalanced classes. The `Class' feature is imbalanced, as
discussed in Section \ref{Data_Overview}. Tomek links and \acrshort{smote} resampling techniques are applied to the
dataset in order to address this imbalance and improve the performance of the \acrshort{knn} classification model.
Additionally, standardisation is applied to scale the feature values, to ensure that all features contribute
equally to the distance calculations and to enhance the accuracy of the model. Standardisation is applied
by the use of Equation \eqref{standardisation}.

\paragraph{\acrshort{knn} model implementation}



\subsection{Classification Tree}

\paragraph{Data preparation}

Data preparation is a crucial step that must be completed before the classification tree algorithm can be applied to the
Dry Bean dataset. If a classification tree algorithm is implemented as described in \ref{CT_background}, then will
the classification tree be robust to outliers, missing values and noise in the dataset.
To prepare the Dry Bean dataset for a classification tree model, the missing values have to be replaced
with \acrshort{nan} and the insatnces that contains missing values in the `Class' feature is dropped. The
classification tree is sensitive to class imbalances. Instead of resampling the data, another technique is implemented
within the classification tree algorithm to enhance the algorithms robustness to class imbalances.

\paragraph{Classification Tree model implementation}

\begin{algorithm}[H]
    \caption{Improved Classification Tree Induction}
    \label{alg:CTI_imporved_algorithm}
    \begin{algorithmic}[1]
        \Function{InduceTree}{$D$, $depth$}
            \If{$\left\lvert D \right\rvert$ = 0}
                \State Return leaf with default class
            \EndIf
            
            \If{$\left\lvert D \right\rvert$ $<$ \text{min\_sample\_split} or $depth$ $>=$ $max depth$}
                \State Return leaf with class label $y_m$, containing $D$
            \EndIf

            \If{$\left\lvert D \right\rvert$ $>$ 0 and $\forall \boldsymbol{\textbf{x}} \in D$ the class is the same, i.e. $y_m$}
                \State Return leaf with class label $y_m$, containing $D$
            \EndIf

            \State Select a test based on a single input variable
            \State Split $D$ into $D_1$,$D_2$,...,$D_o$, where $O$ is the number of outcomes
            \For{$o$ = 1 to $O$}
                \State \Call{InduceTree}{$D_o$, $depth+1$}
            \EndFor
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
    \caption{Post-Pruning a Decision Tree}
    \label{alg:prune_tree}
    \begin{algorithmic}[1]
        \Function{PruneTree}{$X_{\text{val}}$, $y_{\text{val}}$}
            \If{Tree is not trained}
                \State \textbf{raise} ValueError("The tree has not been trained yet.")
            \EndIf
            \State \Call{PruneNode}{root, $X_{\text{val}}$, $y_{\text{val}}$}
        \EndFunction

        \Function{PruneNode}{node, $X_{\text{val}}$, $y_{\text{val}}$}
            \If{node is a leaf node}
                \Return
            \EndIf
            
            \For{each branch in node.branches}
                \If{branch value is numerical}
                    \If{value contains `$>=$'}
                        \State threshold = float(value.replace(`$>= $', `'))
                        \State mask = $X_{\text{val}}[\text{node.test}] \geq \text{threshold}$
                    \Else
                        \State threshold = float(value.replace(`$<$ ', `'))
                        \State mask = $X_{\text{val}}[\text{node.test}] < \text{threshold}$
                    \EndIf
                \Else
                    \State mask = $X_{\text{val}}[\text{node.test}] == \text{value}$
                \EndIf
                
                \State $X_{\text{val\_subset}} = X_{\text{val}}[\text{mask}]$
                \State $y_{\text{val\_subset}} = y_{\text{val}}[\text{mask}]$
                
                \If{$X_{\text{val\_subset}}$ is not empty}
                    \State \Call{PruneNode}{branch, $X_{\text{val\_subset}}$, $y_{\text{val\_subset}}$}
                \EndIf
            \EndFor
            \If{$X_{\text{val}}$ is not empty}
                \State \text{original\_predictions} = \Call{predict}{$X_{\text{val}}$}
                \State \text{original\_f1\_score} = \hfill \Call{f1\_score}{$y_{\text{val}}$, \text{original\_predictions}, \text{average='weighted'}}

                \State \text{prev\_prediction} = node.prediction
                \State \text{original\_branches} = node.branches
                \State node.is\_leaf = \text{True}
                \State node.branches = \{\}
                \State node.prediction = \Call{majority\_class}{$y_{\text{val}}$}
                
                \State \text{pruned\_predictions} = \Call{predict}{$X_{\text{val}}$}
                \State \text{pruned\_f1\_score} = \hfill \Call{f1\_score}{$y_{\text{val}}$, \text{pruned\_predictions}, \text{average='weighted'}}
                \If{\text{pruned\_f1\_score} $<$ \text{original\_f1\_score}}
                    \State node.is\_leaf = \text{False}
                    \State node.branches = \text{original\_branches}
                    \State node.prediction = \text{prev\_prediction}
                \EndIf
            \EndIf
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\section{Empirical Procedure}
\section{Research Results}
\section{Conclusion}
\begin{thebibliography}{00}
    \bibitem{Missing_ref} A. C. Acock "Working with missing values." In: Journal of Marriage and family (2005).
    \bibitem{CART_ref} L. Breiman, J. Friedman, R. A. Olshen and C. J. Stone "Classification and Regression Trees." In: Routledge.  (1984).
    \bibitem{KNN_ref} M. C. Jones, B. W. Silverman “E. Fix and J.L. Hodges (1951): An Important Contribution to Nonparametric Discriminant Analysis and Density Estimation: Commentary on Fix and Hodges (1951).” In: International Statistical Review / Revue Internationale de Statistique, vol. 57, no. 3, (1989).
    \bibitem{ID3_ref} J. R. Quinlan "Induction of decision trees" In: Mach Learn 1 (1986).
    \bibitem{C4.5_ref} J. R. Quinlan "C4. 5: programs for machine learning." In: Elsevier, (1993).
    \bibitem{EDA_ref} J. W. Tukey. "Exploratory data analysis." In: Reading/Addison-Wesley (1977).
\end{thebibliography}


\printglossary[type=\acronymtype]

\clearpage

\begin{table}[h!]
    % \caption{\textbf{Continuous Features}}
    \caption{Continuous Features}
    \begin{center}
    \begin{tabular}{|c||c|c|c|c|c|c|c|c|c|c|}
        \hline
        \textbf{Features}&\textbf{Count}&\textbf{\% Miss.}&\textbf{Card.}&\textbf{Min.}&\textbf{$1^{st}$ Qrt.}&\textbf{Mean}&\textbf{Median}&\textbf{$3^{rd}$ Qrt.}&\textbf{Max}.&\textbf{Std Dev.}\\
        \hline
        %   	                    Count   % Miss     Card.    Min         1st Q       Mean          Median      3rd Q       Max         Std Dev
        \textbf{Area}               &13611  &0.0       &12011   &20420      &36328      &53048.284549 &44652      &61332      &254616     &29324.095717\\
        \textbf{Perimeter}          &13611  &0.0       &13351   &524.736    &703.5235   &855.283459   &794.941    &977.213    &1985.37    &214.289696\\
        \textbf{MajorAxisLength}    &13611  &0.0       &13543   &183.601165 &253.303633 &320.141867   &296.883367 &376.495012 &738.860153 &85.694186\\
        \textbf{MinorAxisLength}    &13611  &0.0       &13543   &122.512653 &175.848170	&202.270714   &192.431733 &217.031741 &460.198497 &44.970091\\
        \textbf{AspectRation}       &13611  &0.0       &13543   &1.024868   &1.432307   &1.583242     &1.551124   &1.707109	  &2.430306   &0.246678\\
        \textbf{Eccentricity}       &13611  &0.0       &13543   &0.218951   &0.715928   &0.750895     &0.764441	  &0.810466	  &0.911423   &0.092002\\
        \textbf{ConvexArea}         &13611  &0.0       &12066   &-30        &36714.5    &53765.692602 &45178      &62294      &263261     &29778.009358\\
        \textbf{EquivDiameter}      &13611  &0.0       &12012   &0.161417   &215.068003 &476.254106   &238.438026 &279.452162 &3014441    &25836.865632\\
        \textbf{Extent}             &13611  &0.044082  &13529   &0.555315   &0.718641   &0.749747     &0.759874   &0.786852	  &0.866195	  &0.049085\\
        \textbf{Solidity}           &13611  &0.0       &13526   &0.919246	&0.985670	&0.987143     &0.988283   &0.990013   &0.994677   &0.004660	\\
        \textbf{Roundness}          &13611  &0.0       &13543   &0.489618   &0.832096   &0.873282     &0.883157   &0.916869   &0.990685   &0.059520\\
        \textbf{Compactness}        &13611  &0.132246  &13525   &0.640577	&0.762577	&0.799886	  &0.801291	  &0.834270   &0.987303   &0.061684\\
        \textbf{ShapeFactor1}       &13611  &0.0       &13543   &0.002778   &0.005900   &0.006564	  &0.006645   &0.007271   &0.010451   &0.001128\\
        \textbf{ShapeFactor2}       &13611  &0.0       &13543   &0.000564	&0.001154	&0.001716	  &0.001694	  &0.002170   &0.003665   &0.000596\\
        \textbf{ShapeFactor3}       &13611  &0.0       &13543   &0.410339   &0.581359	&0.643590     &0.642044	  &0.696006	  &0.974767   &0.098996\\
        \textbf{ShapeFactor4}       &13611  &0.0       &13611   &0.695579	&1.614151	&2.368097	  &2.368757   &3.115695	  &3.966119   &0.871619\\
        \textbf{ShapeFactor5}       &13611  &0.0       &13543   &0.947687   &0.993703   &0.995063	  &0.996386	  &0.997883   &0.999733   &0.004366\\
        \textbf{ShapeFactor6}       &13611  &0.036735  &13606   &0.000466	&45.258826	&89.358603	  &88.76667   &134.273148 &178.985023 &51.838555\\
        \textbf{Sort order}         &13611  &0.0       &13611   &0.000089   &0.248187   &0.500271     &0.50381    &0.750096   &0.999985   &0.287926\\

        \hline
    \end{tabular}
    \label{tab:continuous features}
    \end{center}
\end{table}

\begin{table}[h!]
    % \caption{\textbf{Continuous Features}}
    \caption{Categorical Features}
    \begin{center}
    \begin{tabular}{|c||c|c|c|c|c|c|c|c|c|}
        \hline
        \textbf{Features}&\textbf{Count}&\textbf{\% Miss.}&\textbf{Card.}&\textbf{Mode}&\textbf{Mode Freq.}&\textbf{Mode \%}&\textbf{$2^{nd}$ Mode}&\textbf{$2^{nd}$ Mode Freq.}&\textbf{$2^{nd}$ Mode \%}\\
        \hline
        %   	                    Count   % Miss     Card.    Mode      Mode Freq   Mode %        2nd Mode    2nd Mode Freq  2nd Mode %
        \textbf{Constantness}       &13611  &0.0       &2       &1        &12289      &90.287268     &0          &1322         &9.712732\\
        \textbf{Colour}             &13611  &0.044082  &4       &brown    &6115       &44.926897    &black      &3541          &26.015723\\
        \textbf{Class}              &13611  &0.124899  &7       &DERMASON &3542       &26.02307     &SIRA       &2634          &19.351995\\

        \hline
    \end{tabular}
    \label{tab:categorical features}
    \end{center}
\end{table}

\begin{table}[h!]
    \caption{Hyperparameter tuning for Classification tree}
    \begin{center}
        \begin{tabular}{|c||c|c|c|c|}
            % \hline
            % \textbf{Max depth}
            % \cline{0-5}
                        & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} \\
            \hline
            \textbf{8}  & 94.47388 & 94.47388 & 94.47388 & 94.45916\\
            \textbf{9}  & 94.88595 & 94.86387 & 94.84180 & 94.82708\\
            \textbf{10} & 94.83444 & 94.82708 & 94.74614 & 94.78293\\
            \textbf{11} & 94.75350 & 94.73878 & 94.76821 & 94.79765\\
            \textbf{12} & 94.79029 & 94.76821 & 94.77557 & 94.67255\\
        \end{tabular}
    \end{center}
\end{table}

% \begin{figure}[b!]
%     \centerline{\includegraphics[scale=0.3]{../Plots/Classification tree confusion matrix.png}}
%     \caption{Classification tree confusion matrix}
%     \label{clf confusion matrix}
% \end{figure}
Classification Report:
              precision    recall  f1-score   support

    BARBUNYA       0.86      0.91      0.88       289
      BOMBAY       1.00      1.00      1.00        98
        CALI       0.93      0.89      0.91       323
    DERMASON       0.95      0.98      0.97       698
       HOROZ       0.99      0.99      0.99       383
       SEKER       0.99      0.99      0.99       391
        SIRA       0.97      0.92      0.95       536

    accuracy                           0.96      2718
   macro avg       0.96      0.96      0.96      2718
weighted avg       0.96      0.96      0.96      2718

\end{document}